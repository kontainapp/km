#
# Copyright 2021 Kontain Inc
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
name: KM CI Pipeline
on:
  pull_request:
    branches: [master]
    paths-ignore:
      # See https://docs.github.com/en/actions/reference/workflow-syntax-for-github-actions#filter-pattern-cheat-sheet
      - "**.md" # all .md files in repo
      - "**/docs/**" # all content of all docs/ dirs in repo
      - compile_commands.json
      - .vscode/**
      - km-releases
      - payloads/longhaul-test/**
      - "**/L0-image**"
  push:
    branches: [master]
    paths-ignore:
      - "**.md" # all .md files in repo
      - "**/docs/**" # all content of all docs/ dirs in repo
      - compile_commands.json
      - .vscode/**
      - km-releases
      - payloads/longhaul-test/**
      - "**/L0-image**"

  schedule:
    # Posix cron format:
    # https://pubs.opengroup.org/onlinepubs/9699919799/utilities/crontab.html#tag_20_25_07
    # Minute Hour DayOfMonth MonthOfYear DayOfWeek
    - cron: "0 7 * * *" # Daily build midnight pacific time (UTC + 7)
    # Gitgub doc says:
    #    Scheduled workflows run on the latest commit on the default or base branch.
    #    The shortest interval you can run scheduled workflows is once every 5 minutes.

  # Manual trigger.
  # See https://github.blog/changelog/2020-07-06-github-actions-manual-triggers-with-workflow_dispatch/
  workflow_dispatch:
    inputs:
      run_type:
        description: "Run type: regular or nightly"
        default: "regular"
        required: true

env:
  BUILDENV_IMAGE_VERSION: latest # use this for all buildenv containers
  IMAGE_VERSION: ci-${{ github.run_number }} # use this for all other containers
  SP_SUBSCRIPTION_ID: ${{ secrets.SP_SUBSCRIPTION_ID }}
  SP_APPID: ${{ secrets.SP_APPID }}
  SP_PASSWORD: ${{ secrets.SP_PASSWORD }}
  SP_TENANT: ${{ secrets.SP_TENANT }}
  # TRACE: true # uncomment to enable '-x' in all bash scripts

jobs:
  # starts self-hosted runner in AWS and Azure. They are NOT ephemeral and will run until cleanup in the stop-runner
  start-runners:
    name: Start cloud runners
    runs-on: ubuntu-latest
    outputs:
      run-ons: ${{ steps.start-cloud-runner.outputs.run-ons }}
    steps:
      - name: Get public Keys
        uses: actions/checkout@v2

      - name: Start Cloud runners
        id: start-cloud-runner
        uses: kontainapp/cloud-github-runner@v3.1
        with:
          mode: start
          github-token: ${{ secrets.GH_TOKEN }}
          runner-user: kontain
          subnet: "10.0.0.0"
          public-keys-dir: ${{ github.workspace }}/cloud/ssh
          ec2-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          ec2-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          ec2-region: "us-east-2"
          ec2-image: "L0BaseImage"
          ec2-instance-type: "m5.xlarge"
          ec2-vpc-name: "github-runner"
          az-subscription-id: ${{ secrets.SP_SUBSCRIPTION_ID }}
          az-client-id: ${{ secrets.SP_APPID }}
          az-secret: ${{ secrets.SP_PASSWORD }}
          az-tenant-id: ${{ secrets.SP_TENANT }}
          az-image: "auto-github-runner:L0BaseImage"
          az-location: "westus"
          az-vm-size: "Standard_D4s_v3"

  km-build:
    name: Build KM, push test image
    runs-on: ubuntu-20.04
    steps:
      - uses: actions/checkout@v2
        with:
          submodules: true

      - name: Print build environment info
        run: |
          echo "Event: ${{ github.event_name }} inputs.run_type: ${{ github.event.inputs.run_type }}"
          echo ====Environment info===
          echo "SHA: $(git rev-parse HEAD)"
          echo "=== Last 10 commits:"
          git log -n 10 --graph --pretty=format:'%h% %d %s %cr %ce'
          echo "=== VM/OS:"
          cat /proc/version
          cat /etc/os-release
          echo "=== Docker version:"
          docker version
          echo ==== Environment Variables
          env
          echo ==== CPU Info
          lscpu

      - run: make -C cloud/azure login-cli

      - name: Prepare KM build env
        run: make -C tests pull-buildenv-image .buildenv-local-lib

      - name: Check clang-format on source code
        run: make withdocker TARGET=clang-format-check

      - name: Build KM and tests using buildenv image
        run: make -j withdocker RUN_IN_CI=1

      - name: Build KM for coverage
        run: make -C km -j withdocker TARGET=coverage
        if: env.TEST_COVERAGE == 'true'

      - name: Build and push KM testenv image
        run: make -C tests testenv-image push-testenv-image

      - name: Build payloads and create test images
        run: |
          make -C payloads -j pull-buildenv-image
          make -C payloads -j clean
          make -C payloads -j all
          make -C payloads -j testenv-image
          make -C payloads -j push-testenv-image

      - name: Create payloads runenv and demo-runenv images
        # Note: this builds both runenv and demo-runenv images
        run: |
          make -C payloads -j runenv-image
          make -C payloads -j push-demo-runenv-image

      # Note: custom Python build needs to happen after python payload build
      - name: Python.km custom build
        run: |
          make -C payloads/python build-modules pack-modules CUSTOM_MODULES="markupsafe wrapt"
          make -C payloads/python custom CUSTOM_MODULES="markupsafe wrapt"

      - name: Build faktory
        run: make -C tools/faktory all

      - name: Build shim
        run: |
          make -C cloud/k8s/deploy/shim
          mkdir /opt/kontain/bin/shim
          cp -r build/cloud/k8s/deploy/shim/* /opt/kontain/bin/shim

      - uses: actions/upload-artifact@v2
        with:
          name: km
          path: |
            /opt/kontain/bin/km
            /opt/kontain/bin/krun
            /opt/kontain/bin/shim
          retention-days: 7

      - uses: actions/upload-artifact@v2
        with:
          name: km-build
          path: |
            build/cloud/k8s/deploy/shim
            build/km/km
            tests/hello_test.km
            build/container-runtime/krun-label-trigger
            container-runtime/docker_config.sh

          retention-days: 7

  # Builds a static linked, stripped binary for krun/crun
  # This uses the NIX virtual machine, just like the crun CI.
  # NIX takes a long time, so we build the static krun binary in
  # parallel with the rest of KM build and test.
  krun-static-build:
    name: Build static KRUN binary
    runs-on: ubuntu-20.04
    steps:
      - uses: actions/checkout@v2
        with:
          submodules: recursive
      - name: Set env
        run: echo "CRUN_SHA=$(cd container-runtime/crun/ && git rev-parse --short=12 HEAD)" >> $GITHUB_ENV

      # get persistent artifact that contains static krun
      # it will set step's output variable artifact-status to either 'available' or 'not-found'
      # Note: to figure out where krun needs to be rebuild ot not we use SHA of the container-runtime/crun/ directory
      # in teh step above. That SHA becomes artifact name. If there were changes to krun, SHA will change
      # forcing the re-build
      - name: check artifact
        id: check-artifact
        uses: kontainapp/persistent-artifact@v1
        with:
          github-token: ${{ secrets.GH_TOKEN }}
          artifact-name: ${{ env.CRUN_SHA}}
          path: /tmp/krun-static
          debug: true

      # if persistent artifact was not found, i.e output of the previous step artifact-status != 'available'
      # build krun
      - uses: actions/cache@v2
        if: ${{ steps.check-artifact.outputs.artifact-status != 'available' }}
        with:
          path: .cache
          key: nix-v1-${{ hashFiles('container-runtime/crun/nix/nixpkgs.json') }}

      - name: Build krun
        if: ${{ steps.check-artifact.outputs.artifact-status != 'available' }}
        run: |
          set -x
          CRUN_DIR=$(pwd)/container-runtime/crun
          # These next two lines were taken from the crun release.yaml and build-aux/release.sh
          # to setup and execute a nix based build of stripped static
          if [[ -z $(ls -A /nix) ]]; then sudo docker run --rm --privileged -v /:/mnt nixos/nix:2.3.12 cp -rfT /nix /mnt/nix; fi
          sudo docker run --rm --privileged -v /nix:/nix -v ${CRUN_DIR}:${CRUN_DIR} -w ${CRUN_DIR} nixos/nix:2.3.12 nix build --print-build-logs --file nix/
          mkdir -p /tmp/krun-static
          cp ${CRUN_DIR}/result/bin/crun /tmp/krun-static/krun.static

      # if persistent artifact was not found, i.e output of the previous step artifact-status != 'available'
      # and was built in the previous step, upload it with SHA as artifact name
      - uses: actions/upload-artifact@v2
        if: ${{ steps.check-artifact.outputs.artifact-status != 'available' }}
        with:
          name: ${{ env.CRUN_SHA}}
          path: /tmp/krun-static

      # regardles how we got krun-static (by retrieving persistent artifact or bulding)
      # upload it as krun-static artifact
      - uses: actions/upload-artifact@v2
        with:
          name: krun-static
          path: /tmp/krun-static/krun.static

  # Build the k8s release artifact. Static KRUN is built separately and
  # in parallel from the rest of KM. The k8s artifact contains:
  #  km - km executable
  #  containerd-shim-krun-v2 - containerd shim for krun
  #  krun - krun executable
  k8s-release-bin:
    name: make k8s release bundle
    runs-on: ubuntu-20.04
    needs: [km-build, krun-static-build]
    steps:
      - uses: actions/checkout@v2
        with:
          submodules: true

      # get build directory created by km-build job
      - uses: actions/download-artifact@v2
        with:
          name: km-build
          path: .

      #get static krun created by krun-static-build job
      - uses: actions/download-artifact@v2
        with:
          name: krun-static
          path: build/container-runtime

      - name: rename rkun.static to krun
        run: |
          set -x
          # rename rkun.static to krun
          mv build/container-runtime/krun.static build/container-runtime/krun
          mkdir -p build/opt_kontain/bin
          mv container-runtime/docker_config.sh build/opt_kontain/bin/docker_config.sh

      - run: make -C cloud/azure login-cli

      - name: Prepare KM build env
        run: make -C tests pull-buildenv-image .buildenv-local-lib

      - name: Create kkm release artifact
        run: make withdocker TARGET=kkm-pkg

      - name: Checking download and creating build directory
        run: |
          ls -R build

      # Note: need to have kkm.run ready for this step
      - name: Create kontain bundles
        run: |
          BLDTOP=$(realpath build)

          chmod 755 ${BLDTOP}/km/km
          chmod 755 ${BLDTOP}/container-runtime/krun
          chmod 755 ${BLDTOP}/cloud/k8s/deploy/shim/containerd-shim-krun-v2

          tar -czvf ${BLDTOP}/kontain_bin.tar.gz \
              -C ${BLDTOP} km/km container-runtime/krun container-runtime/krun-label-trigger \
              cloud/k8s/deploy/shim/containerd-shim-krun-v2 kkm.run \
              -C ${BLDTOP}/opt_kontain bin/docker_config.sh

      - uses: actions/upload-artifact@v2
        with:
          name: km-k8s-rel-artifact
          path: build/kontain_bin.tar.gz

  km-test:
    name: KM, payloads, krun tests, KVM on Azure
    runs-on: ${{ fromJSON(needs.start-runners.outputs.run-ons)['azure'] }}
    needs: [start-runners, km-build]
    steps:
      - uses: actions/checkout@v2
        with:
          submodules: true

      - run: make -C cloud/azure login-cli

      - uses: actions/download-artifact@v2
        with:
          name: km
          path: /tmp/

      - name: Configure KRUN
        run: |
          sudo mkdir -p /opt/kontain/bin
          sudo mv -t /opt/kontain/bin /tmp/km /tmp/krun
          sudo chmod +x /opt/kontain/bin/km /opt/kontain/bin/krun
          sudo bash ./container-runtime/podman_config.sh
          sudo bash ./container-runtime/docker_config.sh
          echo 'KERNEL=="kvm", GROUP="kvm", MODE="0666"' > /tmp/rules
          sudo mv /tmp/rules /etc/udev/rules.d/99-perm.rules
          sudo udevadm control --reload-rules && sudo udevadm trigger

      - name: Pull testenv images
        run: |
          make -C tests pull-testenv-image
          make -C payloads -j pull-testenv-image

      - name: KM with KVM Test - Azure
        run: make -C tests test-withdocker DOCKER_INTERACTIVE= DOCKER_RUN_CLEANUP=
        timeout-minutes: 20

      - name: KM with KVM Payloads Test - Azure
        if: ${{ ! (github.event_name == 'schedule' ||
          (github.event_name == 'workflow_dispatch' && github.event.inputs.run_type == 'nightly')) }}
        run: make -C payloads test-withdocker DOCKER_INTERACTIVE=
        timeout-minutes: 20

      - name: KM with KVM Payloads Test - Azure
        if: ${{ github.event_name == 'schedule' ||
          (github.event_name == 'workflow_dispatch' && github.event.inputs.run_type == 'nightly') }}
        run: make -C payloads test-all-withdocker DOCKER_INTERACTIVE=
        timeout-minutes: 20

      - name: KM with KVM Validate runenv images - Azure
        run: |
          make -C payloads/ -j pull-demo-runenv-image
          make -C payloads validate-runenv-image DOCKER_INTERACTIVE=
        timeout-minutes: 20

  kkm-build:
    name: Build KKM, save test artifacts
    runs-on: ubuntu-20.04
    steps:
      - uses: actions/checkout@v2
        with:
          submodules: true

      - name: Build KKM and KKM tests
        run: make -C kkm/kkm && make -C kkm/test_kkm

      - name: Install KKM
        run: sudo insmod kkm/kkm/kkm.ko

      - name: Sanity check - unit test
        run: ./kkm/test_kkm/test_kkm

      - uses: actions/upload-artifact@v2
        with:
          name: kkm
          path: |
            kkm/kkm/kkm.ko
            kkm/test_kkm/test_kkm
          retention-days: 7

  # TODO: Maybe not needed - review
  kkm-test:
    name: KM tests, KKM on CI VM
    runs-on: ubuntu-20.04
    needs: [km-build, kkm-build]
    steps:
      - uses: actions/checkout@v2
        with:
          submodules: true
      - run: make -C cloud/azure login-cli

      - uses: actions/download-artifact@v2
        with:
          name: kkm
          path: kkm
      - run: chmod a+x kkm/test_kkm/test_kkm

      - name: Install KKM
        run: sudo insmod kkm/kkm/kkm.ko

      - name: Pull testenv image
        run: make -C tests pull-testenv-image

      - name: KM Tests - locally with KKM
        run: make -C tests test-withdocker HYPERVISOR_DEVICE=/dev/kkm DOCKER_INTERACTIVE=
        timeout-minutes: 15

      - name: Get kernel logs
        if: always()
        run: sudo dmesg

  kkm-test-vms:
    name: KM tests, KKM AWS
    needs: [start-runners, km-build, kkm-build]
    runs-on: ${{ fromJSON(needs.start-runners.outputs.run-ons)['ec2'] }}
    steps:
      - uses: actions/checkout@v2
        with:
          submodules: true

      - run: make -C cloud/azure login-cli

      - uses: actions/download-artifact@v2
        with:
          name: km
          path: /tmp/

      - name: KKM and KKM tests
        run: |
          make -C kkm/kkm && make -C kkm/test_kkm
          sudo insmod kkm/kkm/kkm.ko
          ./kkm/test_kkm/test_kkm
          sudo mkdir -p /opt/kontain/bin
          sudo mv -t /opt/kontain/bin /tmp/km /tmp/krun
          sudo chmod +x /opt/kontain/bin/km /opt/kontain/bin/krun
          sudo bash ./container-runtime/podman_config.sh
          sudo bash ./container-runtime/docker_config.sh

      - name: Pull testenv images
        run: |
          make -C tests pull-testenv-image
          make -C payloads -j pull-testenv-image

      - name: KM with KKM Test - AWS
        run: make -C tests test-withdocker HYPERVISOR_DEVICE=/dev/kkm DOCKER_INTERACTIVE= DOCKER_RUN_CLEANUP=
        timeout-minutes: 20

      - name: KM with KKM Payloads Test - AWS
        if: ${{ ! (github.event_name == 'schedule' ||
          (github.event_name == 'workflow_dispatch' && github.event.inputs.run_type == 'nightly')) }}
        run: make -C payloads test-withdocker HYPERVISOR_DEVICE=/dev/kkm DOCKER_INTERACTIVE=
        timeout-minutes: 20

      - name: KM with KKM Payloads Test - AWS (nightly)
        if: ${{ github.event_name == 'schedule' ||
          (github.event_name == 'workflow_dispatch' && github.event.inputs.run_type == 'nightly') }}
        run: make -C payloads test-all-withdocker HYPERVISOR_DEVICE=/dev/kkm DOCKER_INTERACTIVE=
        timeout-minutes: 20

      - name: KM with KKM Validate runenv images - AWS
        run: |
          make -C payloads/ -j pull-demo-runenv-image
          make -C payloads validate-runenv-image HYPERVISOR_DEVICE=/dev/kkm DOCKER_INTERACTIVE=
        timeout-minutes: 20

  minikube-testing:
    name: kubernetes tests against minikube
    runs-on: ubuntu-20.04
    needs: [k8s-release-bin, kkm-build]
    strategy:
      matrix:
        # We want to test against containerd and crio. crio support is a bit brittle in
        # minikube. In our experience (late 2021) it works better with the podman minikube
        # driver than the docker minikube driver.
        runtime: ["containerd"] # "cri-o" later
        driver: ["docker", "podman"]
    steps:
      - uses: actions/checkout@v2
        with:
          submodules: true

      - uses: actions/download-artifact@v2
        with:
          name: kkm
          path: kkm
      - run: chmod a+x kkm/test_kkm/test_kkm

      - uses: actions/download-artifact@v2
        with:
          name: km-k8s-rel-artifact
          path: /tmp/bin_release

      - name: print downloaded artifact
        run: find /tmp/bin_release

      - name: start local file server
        run: |
          (cd /tmp/bin_release; python3 -m http.server 8000) &

      - name: Install KKM
        run: sudo insmod kkm/kkm/kkm.ko

      - name: Start minikube
        run: |
          minikube version
          minikube start --container-runtime=${{ matrix.runtime }} --driver=${{ matrix.driver }} --wait=all || minikube logs

      - name: Install Kontain on minikube k8s
        run: |
          # merges and applies all the files for CI overlay
          export RELEASE_TAG=current && envsubst < cloud/k8s/deploy/kontain-deploy/base/set_env.templ > cloud/k8s/deploy/kontain-deploy/base/set_env.yaml
          kubectl apply -k cloud/k8s/deploy/kontain-deploy/overlays/ci
          # kubectl apply -f cloud/k8s/deploy/runtime-class.yaml
          # kubectl apply -f cloud/k8s/deploy/cm-install-lib.yaml
          # kubectl apply -f cloud/k8s/deploy/cm-containerd-install.yaml
          # kubectl apply -k cloud/k8s/deploy/kontain-deploy/ci
          sleep 20

      # uncomment to get logs from damonset.
      #- name: Check k8s install
      #  run: |
      #    set -x
      #    kubectl get pod -A
      #    kubectl logs -n kube-system -l app=kontain-init

      - name: Run user test
        run: |
          # start the test pod, which will just sleep
          kubectl apply -f demo/k8s/test.yaml
          sleep 10
          # exec `uname -r` on the test pod. Should get back '.kontain.KKM' appended to the linux release name
          pname=$(kubectl get pod -l kontain=test-app -o jsonpath="{.items[0].metadata.name}")
          rname=$(kubectl exec "${pname}" -- uname -r)
          echo "uname -r returned ${rname}"
          echo "${rname}" | grep -q -e '.*\.kontain\.KKM$' || exit 1
          echo "Kontain Runtime OK".

  ecs-hack-test:
    name: test alternate KM triggers
    runs-on: ubuntu-20.04
    needs: [k8s-release-bin, kkm-build]
    steps:
      - uses: actions/checkout@v2
        with:
          submodules: true

      - uses: actions/download-artifact@v2
        with:
          name: kkm
          path: kkm
      - run: chmod a+x kkm/test_kkm/test_kkm

      - uses: actions/download-artifact@v2
        with:
          name: km-k8s-rel-artifact
          path: /tmp/bin_release

      - name: Install KKM
        run: sudo insmod kkm/kkm/kkm.ko

      - name: Untar release
        run: tar -C /tmp/bin_release -xf /tmp/bin_release/kontain_bin.tar.gz

      - name: Stop Docker
        run: sudo systemctl stop docker

      - name: Install KM and Reconfigure Docker
        run: |
          sudo mkdir -p /opt/kontain/bin
          sudo cp /tmp/bin_release/km/km /opt/kontain/bin/km
          sudo cp /tmp/bin_release/container-runtime/krun /opt/kontain/bin/krun-label-trigger
          sudo cp /tmp/bin_release/cloud/k8s/deploy/shim/containerd-shim-krun-v2 /usr/bin/containerd-shim-krun-v2
          cat << EOF | sudo tee /etc/docker/daemon.json
             {
                 "default-runtime": "krun",
                 "runtimes": {
                     "krun": {
                         "path": "/opt/kontain/bin/krun-label-trigger"
                     }
                 }
             }
          EOF

      - name: restart Docker
        run: |
          sudo systemctl daemon-reload
          sudo systemctl restart docker

      - name: Check krun personality
        run: |
          OUT=$(docker run --rm -v /.kontain busybox uname -r)
          echo ${OUT} | grep -q '.*\.kontain.KKM$'

      - name: Check crun personality
        run: |
          OUT=$(docker run --rm busybox uname -r)
          echo ${OUT} | grep -v -q '.*\.kontain.KKM$'

  codeql-scan:
    # CodeQL Code Scanning/Static Analysis
    name: Analyze
    runs-on: ubuntu-latest
    permissions:
      actions: read
      contents: read
      security-events: write

    container:
      image: kontainapp/buildenv-km-fedora:latest

    strategy:
      fail-fast: false
      matrix:
        language: ["cpp"]

    steps:
      - name: Checkout repository
        uses: actions/checkout@v2

      # Initializes the CodeQL tools for scanning.
      - name: Initialize CodeQL
        uses: github/codeql-action/init@v1
        with:
          languages: ${{ matrix.language }}
          # enhanced security checks.
          queries: security-extended

      - run: |
          make -C km all

      - name: Perform CodeQL Analysis
        uses: github/codeql-action/analyze@v1

  slack-workflow-status:
    name: Notify slack, if needed
    runs-on: ubuntu-latest
    # 'contains' shows how to add conditions, e.g. on workflow 'name:', or many other contexts.
    # see https://docs.github.com/en/actions/reference/context-and-expression-syntax-for-github-actions
    if: (failure() && github.ref == 'refs/heads/master') ||
      contains(github.workflow, 'noisy')
    # Dependencies. (A skipped dependency is considered satisfied)
    needs:
      [
        km-build,
        krun-static-build,
        kkm-build,
        km-test,
        kkm-test,
        kkm-test-vms,
        minikube-testing,
        codeql-scan,
        ecs-hack-test,
      ]
    steps:
      - name: Send notification to slack
        uses: Gamesight/slack-workflow-status@master
        with:
          repo_token: ${{ secrets.GITHUB_TOKEN }}
          slack_webhook_url: ${{ secrets.SLACK_WEBHOOK_URL }}
          include_jobs: true
          channel: "#build_and_test"
          name: "CI workflow status"
          icon_emoji: ":thumbsdown:"

  # This step is to clean up on-demand runner. They work in conjunction with start-runner.
  # Make sure to add dependencies in both "needs" clauses
  stop-runner:
    if: always()
    name: Terminate self-hosted cloud runners
    needs: [start-runners, km-test, kkm-test-vms]
    runs-on: ubuntu-latest
    steps:
      - name: Stop runners
        uses: kontainapp/cloud-github-runner@v3.1
        with:
          mode: stop
          status: ${{ toJSON(needs) }}
          github-token: ${{ secrets.GH_TOKEN }}
          ec2-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          ec2-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          ec2-region: "us-east-2"
          az-subscription-id: ${{ secrets.SP_SUBSCRIPTION_ID }}
          az-client-id: ${{ secrets.SP_APPID }}
          az-secret: ${{ secrets.SP_PASSWORD }}
          az-tenant-id: ${{ secrets.SP_TENANT }}
