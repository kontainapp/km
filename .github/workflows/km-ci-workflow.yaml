#
# Copyright 2021 Kontain Inc
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
name: KM CI Pipeline
on:
  pull_request:
    branches: [master]
    paths-ignore:
      # See https://docs.github.com/en/actions/reference/workflow-syntax-for-github-actions#filter-pattern-cheat-sheet
      - "**.md" # all .md files in repo
      - "**/docs/**" # all content of all docs/ dirs in repo
      - compile_commands.json
      - .vscode/**
      - km-releases
      - payloads/longhaul-test/**
      - "**/L0-image**"
  push:
    branches: [master]
    paths-ignore:
      - "**.md" # all .md files in repo
      - "**/docs/**" # all content of all docs/ dirs in repo
      - compile_commands.json
      - .vscode/**
      - km-releases
      - payloads/longhaul-test/**
      - "**/L0-image**"

  schedule:
    # Posix cron format:
    # https://pubs.opengroup.org/onlinepubs/9699919799/utilities/crontab.html#tag_20_25_07
    # Minute Hour DayOfMonth MonthOfYear DayOfWeek
    - cron: "0 7 * * *" # Daily build midnight pacific time (UTC + 7)
    # Gitgub doc says:
    #    Scheduled workflows run on the latest commit on the default or base branch.
    #    The shortest interval you can run scheduled workflows is once every 5 minutes.

  # Manual trigger.
  # See https://github.blog/changelog/2020-07-06-github-actions-manual-triggers-with-workflow_dispatch/
  workflow_dispatch:
    inputs:
      run_type:
        description: "Run type: regular or nightly"
        default: "regular"
        required: true

env:
  BUILDENV_IMAGE_VERSION: latest # use this for all buildenv containers
  IMAGE_VERSION: ci-${{ github.run_number }} # use this for all other containers
  NIGHTLY_CLUSTER_NAME: "aks-kontain-nightly-ci-${{ github.run_number }}"
  SP_SUBSCRIPTION_ID: ${{ secrets.SP_SUBSCRIPTION_ID }}
  SP_APPID: ${{ secrets.SP_APPID }}
  SP_PASSWORD: ${{ secrets.SP_PASSWORD }}
  SP_TENANT: ${{ secrets.SP_TENANT }}
  # TRACE: true # uncomment to enable '-x' in all bash scripts

jobs:
  km-build:
    name: Build KM, push test image
    runs-on: ubuntu-20.04
    steps:
      - uses: actions/checkout@v2
        with:
          submodules: true

      - name: Print build environment info
        run: |
          echo "Event: ${{ github.event_name }} inputs.run_type: ${{ github.event.inputs.run_type }}"
          echo ====Environment info===
          echo "SHA: $(git rev-parse HEAD)"
          echo "=== Last 10 commits:"
          git log -n 10 --graph --pretty=format:'%h% %d %s %cr %ce'
          echo "=== VM/OS:"
          cat /proc/version
          cat /etc/os-release
          echo "=== Docker version:"
          docker version
          echo ==== Environment Variables
          env
          echo ==== CPU Info
          lscpu

      - name: verify clang-format compliance
        run: |
          clang-format --style=file -n -Werror km/*.h km/*.c tests/*.h tests/*.c tests/*.cpp

      - run: make -C cloud/azure login-cli

      - name: Prepare KM build env
        run: make -C tests pull-buildenv-image .buildenv-local-lib

      - name: Build KM and tests using buildenv image
        run: make -j withdocker

      - name: Build KM for coverage
        run: make -C km -j withdocker TARGET=coverage
        if: env.TEST_COVERAGE == 'true'

      - name: Build and push KM testenv image
        run: make -C tests testenv-image push-testenv-image

        # Note: we need to have km built before kontaind.
      - name: Build and push kontaind image
        run: make -C cloud/k8s/kontaind push-runenv-image

      - name: Build payloads and create test images
        run: make -C payloads pull-buildenv-image clean all testenv-image

      - run: make -C payloads push-testenv-image

      - name: Create payloads runenv and demo-runenv images
        # Note: this builds both runenv and demo-runenv images
        run: make -C payloads runenv-image

      - run: make -C payloads push-demo-runenv-image

      # Note: custom Python build needs to happen after python payload build
      - name: Python.km custom build
        run: |
          make -C payloads/python build-modules pack-modules CUSTOM_MODULES="markupsafe wrapt"
          make -C payloads/python custom CUSTOM_MODULES="markupsafe wrapt"

      - name: Build faktory
        run: make -C tools/faktory all

      - name: Build shim
        run: |
          make -C cloud/k8s/deploy/shim
          mkdir /opt/kontain/bin/shim
          cp -r build/cloud/k8s/deploy/shim/* /opt/kontain/bin/shim

      - uses: actions/upload-artifact@v2
        with:
          name: km
          path: |
            /opt/kontain/bin/km
            /opt/kontain/bin/krun
            /opt/kontain/bin/shim
          retention-days: 7

      - name: capture submodules
        run: git submodule status > /tmp/km_submodule_status

      - uses: actions/upload-artifact@v2
        with:
          name: km_submodule_status
          path: /tmp/km_submodule_status


  # Builds a static linked, stripped binary for krun/crun
  # This uses the NIX virtual machine, just like the crun CI.
  # NIX takes a long time, so we build the static krun binary in
  # parallel with the rest of KM build and test. We just the 
  krun-static-build:
    name: Build static KRUN binary
    runs-on: ubuntu-20.04
    steps:
      - uses: actions/checkout@v2
        with:
          submodules: recursive

      - uses: actions/cache@v2
        with:
          path: .cache
          key: nix-v1-${{ hashFiles('container-runtime/crun/nix/nixpkgs.json') }}

      - run: |
          set -x
          CRUN_DIR=$(pwd)/container-runtime/crun
          # These next two lines were taken from the crun release.yaml and build-aux/release.sh
          # to setup and execute a nix based build of stripped static
          if [[ -z $(ls -A /nix) ]]; then sudo docker run --rm --privileged -v /:/mnt nixos/nix:2.3.12 cp -rfT /nix /mnt/nix; fi
          sudo docker run --rm --privileged -v /nix:/nix -v ${CRUN_DIR}:${CRUN_DIR} -w ${CRUN_DIR} nixos/nix:2.3.12 nix build --print-build-logs --file nix/ --arg enableSystemd false
          mkdir -p /tmp/krun-static
          cp ${CRUN_DIR}/result/bin/crun /tmp/krun-static/krun.static

      - uses: actions/upload-artifact@v2
        with:
          name: krun-static
          path: /tmp/krun-static


  # Build the k8s release artifact. Static KRUN is built separately and 
  # in parallel from the rest of KM. The k8s artifact contains:
  #  km - km execuatble
  #  containerd-shim-krun-v2 - cintainerd shim for krun
  #  krun - krun executable
  #  km_submodule_status - git submodule status for KM. Contains required KKM SHA.
  k8s-release-bin:
    name: make k8s release bundle
    runs-on: ubuntu-20.04
    needs: [km-build, krun-static-build]
    steps:
      - uses: actions/download-artifact@v2
        with:
          name: km
          path: /tmp/km
      - uses: actions/download-artifact@v2
        with:
          name: km_submodule_status
          path: /tmp/km_submodule_status
      - uses: actions/download-artifact@v2
        with:
          name: krun-static
          path: /tmp/krun-static
      - run: |
          set -x
          mkdir /tmp/k8s-release-bin
          find /tmp/km
          find /tmp/km_submodule_status
          find /tmp/krun-static
          mkdir -p /tmp/k8s-release-bin/km
          cp /tmp/km/km /tmp/k8s-release-bin/km/km
          chmod 755 /tmp/k8s-release-bin/km/km
          mkdir -p /tmp/k8s-release-bin/container-runtime
          cp /tmp/krun-static/krun.static /tmp/k8s-release-bin/container-runtime/krun
          chmod 755 /tmp/k8s-release-bin/container-runtime/krun
          mkdir -p /tmp/k8s-release-bin/cloud/k8s/deploy/shim
          cp /tmp/km/shim/containerd-shim-krun-v2 /tmp/k8s-release-bin/cloud/k8s/deploy/shim/containerd-shim-krun-v2
          chmod 755 /tmp/k8s-release-bin/cloud/k8s/deploy/shim/containerd-shim-krun-v2
          cp /tmp/km_submodule_status/km_submodule_status /tmp/k8s-release-bin/km_submodule_status
          chmod 444 /tmp/k8s-release-bin/km_submodule_status
          tar -C /tmp/k8s-release-bin -czf /tmp/km-k8s-release-artifact.tar.gz .
      - uses: actions/upload-artifact@v2
        with:
          name: km-k8s-rel-artifact
          path: /tmp/km-k8s-release-artifact.tar.gz


  km-test:
    name: KM, payloads, krun tests, KVM on K8S
    runs-on: ubuntu-20.04
    needs: km-build
    steps:
      - uses: actions/checkout@v2
        with:
          submodules: true

      - run: make -C cloud/azure login-cli

      - name: KM Tests on K8S with KVM
        run: make -C tests test-withk8s TEST_JOBS=4
        timeout-minutes: 15

      - name: Test payloads on K8S with KVM
        run: make -C payloads test-withk8s
        timeout-minutes: 10


  km-test-withpacker:
    name: Basic payload tests, KVM Azure VM
    runs-on: ubuntu-latest
    needs: km-build
    steps:
      - uses: actions/checkout@v2
        with:
          submodules: true

      - run: make -C cloud/azure login-cli

      - uses: actions/download-artifact@v2
        with:
          name: km
          path: /tmp/

      - name: Payload tests with packer
        # to avoid calling packer in a loop, we call it on one dir (does not matter which one)
        # and pass the parent to it, so that the scan of payloads will happen inside a single docker
        run: make -C payloads/busybox test-withpacker PACKER_DIR=payloads TIMEOUT=15m STEP=test
        # note: not using CI's 'timeout-minutes' to avoid a kill without cleaning resources

  kkm-build:
    name: Build KKM, save test artifacts
    runs-on: ubuntu-20.04
    steps:
      - uses: actions/checkout@v2
        with:
          submodules: true

      - name: Build KKM and KKM tests
        run: make -C kkm/kkm && make -C kkm/test_kkm

      - name: Install KKM
        run: sudo insmod kkm/kkm/kkm.ko

      - name: Sanity check - unit test
        run: ./kkm/test_kkm/test_kkm

      - uses: actions/upload-artifact@v2
        with:
          name: kkm
          path: |
            kkm/kkm/kkm.ko
            kkm/test_kkm/test_kkm
          retention-days: 7

  kkm-test:
    name: KM tests, KKM on CI VM
    runs-on: ubuntu-20.04
    needs: [km-build, kkm-build]
    steps:
      - uses: actions/checkout@v2
        with:
          submodules: true
      - run: make -C cloud/azure login-cli

      - uses: actions/download-artifact@v2
        with:
          name: kkm
          path: kkm
      - run: chmod a+x kkm/test_kkm/test_kkm

      - name: Install KKM
        run: sudo insmod kkm/kkm/kkm.ko

      - name: Pull testenv image
        run: make -C tests pull-testenv-image

      - name: KM Tests - locally with KKM
        run: make -C tests test-withdocker HYPERVISOR_DEVICE=/dev/kkm DOCKER_INTERACTIVE=
        timeout-minutes: 15

      - name: Get kernel logs
        if: always()
        run: sudo dmesg

  kkm-test-vms:
    name: KM tests, KKM AWS and Azure VMs
    runs-on: ubuntu-20.04
    needs: [km-build, kkm-build]
    steps:
      - uses: actions/checkout@v2

      - uses: actions/download-artifact@v2
        with:
          name: km
          path: /tmp/

      - name: KM with KKM Test - AWS and Azure with packer
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        run: make -C tests test-withpacker HYPERVISOR_DEVICE=/dev/kkm TIMEOUT=20m STEP=test-vms
        # note: not using CI's 'timeout-minutes' to avoid a kill without cleaning resources

  km-krun-validate-runenv:
    name: Validate runenv, KVM Azure VM
    runs-on: ubuntu-20.04
    needs: km-build
    steps:
      - uses: actions/checkout@v2

      - uses: actions/download-artifact@v2
        with:
          name: km
          path: /tmp/

      - name: Validate runenv images on Azure
        run: make -C tests validate-runenv-image-withpacker TIMEOUT=20m PACKER_DIR=payloads STEP=validate-runenv
        # note: not using CI's 'timeout-minutes' to avoid a kill without cleaning resources

  km-krun-kkm-validate-runenv:
    name: Validate runenv, KKM AWS and Azure VMs
    runs-on: ubuntu-20.04
    needs: [km-build, kkm-build]
    steps:
      - uses: actions/checkout@v2

      - uses: actions/download-artifact@v2
        with:
          name: km
          path: /tmp/

      - name: Validate runenv images with kkm on AWS and Azure
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        run: make -C tests validate-runenv-image-withpacker HYPERVISOR_DEVICE=/dev/kkm TIMEOUT=20m PACKER_DIR=payloads STEP=kkm-validate-runenv
        # note: not using CI's 'timeout-minutes' to avoid a kill without cleaning resources

  km-k8s-cluster:
    name: Create a new K8S cluster on Azure
    runs-on: ubuntu-20.04
    # The extra tests run if the workflow is triggered on schedule or manually.
    # comment out (in your branch) if you want to force it on your run
    if: ${{ github.event_name == 'schedule' ||
      (github.event_name == 'workflow_dispatch' && github.event.inputs.run_type == 'nightly') }}
    steps:
      - uses: actions/checkout@v2
        with:
          submodules: true
      - run: make -C cloud/azure login-cli

      # We need to create cluster and deploy kontaind to it. However
      # kontaind requires KM build, so we will do the deploy later, before tests.
      # This allows early cluster creation - before a build is completed
      - name: Create k8s Cluster
        run: cloud/azure/aks_ci_create.sh ${{ env.NIGHTLY_CLUSTER_NAME }}

  km-test-all:
    name: Run all tests on a new K8s cluster
    runs-on: ubuntu-20.04
    needs: [km-build, km-k8s-cluster]
    if: ${{ github.event_name == 'schedule' ||
      (github.event_name == 'workflow_dispatch' && github.event.inputs.run_type == 'nightly') }}
    env:
      K8S_TEST_ERR_NO_CLEANUP: true # true means "on error, keep pods around". (assumes cluster is not cleaned up)
    steps:
      - uses: actions/checkout@v2
        with:
          submodules: true
      - run: make -C cloud/azure login-cli
      - run: |
          source cloud/azure/cloud_config.mk
          az aks get-credentials --resource-group "${CLOUD_RESOURCE_GROUP}" --name "${{ env.NIGHTLY_CLUSTER_NAME }}" --overwrite-existing

      - name: Deploy and verify kontaind
        run: make -C cloud/k8s/kontaind deploy

      - name: KM test all, K8s with KVM
        run: make -C tests test-all-withk8s
        timeout-minutes: 15

      - name: Payloads tests, on K8s
        run: make -C payloads test-all-withk8s
        timeout-minutes: 60

      # TODO: https://github.com/kontainapp/km/issues/1326
      # The runenv images assume runtime=krun now.
      # Disable until we figure out how to enable that on AKS.
      # - name: Payloads runenv validation, on K8s
      #   run: make -C payloads validate-runenv-withk8s
      #   timeout-minutes: 5

      - name: KM tests with coverage, on K8s
        # pull buildenv before testing, coverage analysis will need it
        run: make -C tests pull-buildenv-image coverage-withk8s
        timeout-minutes: 15
        # if: always()
        if: env.TEST_COVERAGE == 'true'

      - name: Upload coverage
        run: make -C tests upload-coverage
        if: env.TEST_COVERAGE == 'true'

      - name: Tear down k8s Cluster
        if: always() # replace always() with success() if you want to keep cluster on failure
        run: cloud/azure/aks_ci_destroy.sh ${{ env.NIGHTLY_CLUSTER_NAME }}

      - name: Logout
        if: always()
        run: |
          [ "$TRACE" ] && set -x
          rm -f ~/.kube/config
          az logout

  minikube-testing:
    name: kubernetes tests aginst minikube
    runs-on: ubuntu-20.04
    needs: [k8s-release-bin, kkm-build]
    strategy:
      matrix:
        # We want to test against containerd and crio. crio support is a bit brittle in
        # minikube. In our experience (late 2021) it works better with the podman minikube
        # driver than the docker minikube driver.
        runtime: ["containerd"] # "cri-o" later
        driver: ["docker", "podman"]
    steps:
    - uses: actions/checkout@v2
      with:
        submodules: true

    - uses: actions/download-artifact@v2
      with:
        name: kkm
        path: kkm
    - run: chmod a+x kkm/test_kkm/test_kkm

    - uses: actions/download-artifact@v2
      with:
        name: km-k8s-rel-artifact
        path: /tmp/bin_release

    - name: print downloaded artifact
      run: find /tmp/bin_release

    - name: start local file server
      run: |
        (cd /tmp/bin_release; python3 -m http.server 8000) &

    - name: Install KKM
      run: sudo insmod kkm/kkm/kkm.ko

    - name: Start minikube 
      run: |
        minikube version
        minikube start --container-runtime=${{ matrix.runtime }} --driver=${{ matrix.driver }} --wait=all || minikube logs

    - name: Install Kontain on minikube k8s
      run: |
        kubectl apply -f cloud/k8s/deploy/runtime-class.yaml
        kubectl apply -f cloud/k8s/deploy/cm-install-lib.yaml
        kubectl apply -f cloud/k8s/deploy/cm-containerd-install.yaml
        kubectl apply -k cloud/k8s/deploy/kontain-deploy/ci
        sleep 20

    # uncomment to get logs from damonset.
    #- name: Check k8s install
    #  run: |
    #    set -x
    #    kubectl get pod -A
    #    kubectl logs -n kube-system -l app=kontain-init 

    - name: Run user test
      run: |
        # start the test pod, which will just sleep
        kubectl apply -f demo/k8s/test.yaml
        sleep 10
        # exec `uname -r` on the test pod. Should get back '.kontain.KKM' appended to the linux release name
        pname=$(kubectl get pod -l kontain=test-app -o jsonpath="{.items[0].metadata.name}")
        rname=$(kubectl exec "${pname}" -- uname -r)
        echo "uname -r returned ${rname}"
        echo "${rname}" | grep -q -e '.*\.kontain\.KKM$' || exit 1
        echo "Kontain Runtime OK".

  slack-workflow-status:
    name: Notify slack, if needed
    runs-on: ubuntu-latest
    # 'contains' shows how to add conditions, e.g. on workflow 'name:', or many other contexts.
    # see https://docs.github.com/en/actions/reference/context-and-expression-syntax-for-github-actions
    if: (failure() && github.ref == 'refs/heads/master') ||
      contains(github.workflow, 'noisy')
    # Dependencies. (A skipped dependency is considered satisfied)
    needs: [km-build, krun-static-build, kkm-build, km-test, km-test-withpacker, kkm-test, kkm-test-vms, km-krun-validate-runenv, km-krun-kkm-validate-runenv, km-test-all]
    steps:
      - name: Send notification to slack
        uses: Gamesight/slack-workflow-status@master
        with:
          repo_token: ${{ secrets.GITHUB_TOKEN }}
          slack_webhook_url: ${{ secrets.SLACK_WEBHOOK_URL }}
          include_jobs: true
          channel: "#build_and_test"
          name: "CI workflow status"
          icon_emoji: ":thumbsdown:"
